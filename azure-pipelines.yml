# Python Django
# Test a Django project on multiple versions of Python.
# Add steps that analyze code, save build artifacts, deploy, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- master

pool:
  #name: TradeMetrics-Build
  vmImage: 'ubuntu-latest'


strategy:
  matrix:
    Python37:
      PYTHON_VERSION: '3.7'
  maxParallel: 3

variables:
  DEBIAN_FRONTEND: 'noninteractive'

steps:

- script: |
    set -x
    # The total size of the env variables must be less than ARG_MAX
    # or you will get an "Argument list too long" error.
    # This is normally 2097152 bytes (2,048 KB). A single env variable
    # is must be no greater than 1/16 of ARG_MAX or 128 KB.
    getconf ARG_MAX
    env | wc -c
    echo "TESTPATH=$TESTPATH" 
    echo "DEBIAN_FRONTEND=$DEBIAN_FRONTEND" 
  displayName: 'Display env'
  env:
    TESTPATH: '$(System.DefaultWorkingDirectory)/fakebin'

- script: |
    echo "Test script"
    date
    ls
    pwd
    which env
    echo "GLOB TEST:" *
    env | sort
  displayName: 'Test script task'
  env:
    PATH: '$(System.DefaultWorkingDirectory)/fakebin:/usr/share/rust/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'

- script: |
    DEBIAN_FRONTEND=noninteractive apt update
    DEBIAN_FRONTEND=noninteractive apt install -y wget bzip2
    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
    rm -Rf /root/miniconda3
    bash Miniconda3-latest-Linux-x86_64.sh -b
    source $HOME/miniconda3/bin/activate
    conda env update -q -f server/environment.yml > /dev/null
    source /home/vsts/miniconda3/etc/profile.d/conda.sh
    conda activate
    python -m pip install --upgrade pip setuptools wheel
    pip install unittest-xml-reporting
  displayName: 'Install prerequisites'

- script: |
    pushd 'server'
    source /home/vsts/miniconda3/etc/profile.d/conda.sh
    conda activate
    #python manage.py test --testrunner xmlrunner.extra.djangotestrunner.XMLTestRunner --no-input
    pytest -vv
  displayName: 'Run tests'

- task: PublishTestResults@2
  inputs:
    testResultsFiles: "**/test-*.xml"
    testRunTitle: 'Python $(PYTHON_VERSION)'
  condition: succeededOrFailed()

- task: PublishCodeCoverageResults@1
  inputs:
    # pytest-cov creates a coverage.xml file in the same format as Cobertura.
    codeCoverageTool: Cobertura
    summaryFileLocation: '**/coverage.xml'
    reportDirectory: '**/htmlcov'
  condition: succeededOrFailed()
